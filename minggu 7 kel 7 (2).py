# -*- coding: utf-8 -*-
"""Big_Project_Progress_Python_Minggu_Ketujuh__Kelompok_7 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kG1tRPFPJ4UfmiDpjm4_pv85S7mg7mhR

# **Kelompok 7**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

# Library untuk visualisasi data
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, OneHotEncoder, OrdinalEncoder

# for resampling
from imblearn.over_sampling import RandomOverSampler

#for MODEL
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

#for checking testing results
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix

import csv
import os

"""**Membaca** **Dataset**"""

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('data (1).csv')
df

data = df

data.info()

data.describe()

#Melabeli data yang ada di variabel kategorik menjadi numerik
data[['diagnosis']] = data[['diagnosis']].apply(LabelEncoder().fit_transform)

#view udpated DataFr
data

"""**Visualisasi** **Dataset**"""

data['diagnosis'].unique()

import matplotlib.pyplot as plt
# Menghitung jumlah data untuk setiap kategori pada variabel kategorik
kategori_counts = data['diagnosis'].value_counts()

# Plotting pie chart dengan jumlah data dan persentase
plt.figure(figsize=(8, 8))
plt.pie(kategori_counts, labels=[f'{label} ({count} data, {count / sum(kategori_counts) * 100:.1f}%)' for label, count in zip(kategori_counts.index, kategori_counts)], autopct='', startangle=140)
plt.title('Pie Chart Variabel Diagnosis')
plt.show()

X=data.drop(columns=["diagnosis", "radius_se","texture_se","perimeter_se","area_se","smoothness_se",
                     "compactness_se","concavity_se","concave points_se","symmetry_se","fractal_dimension_se",
                     "radius_worst","texture_worst","perimeter_worst","area_worst","smoothness_worst",
                     "compactness_worst","concavity_worst","concave points_worst","symmetry_worst",
                     "fractal_dimension_worst"])
Y=data["diagnosis"]

X.shape

Y.shape

# initialize figure with 11 subplots in a row
fig, ax = plt.subplots(1, 11, figsize=(50, 35))

# add padding between the subplots
plt.subplots_adjust(wspace=0.5)

# draw boxplot for age in the 1st subplot
sns.boxplot(data=data['radius_mean'], ax=ax[0], color='brown',)
ax[0].set_xlabel('radius_mean')

# draw boxplot for station_distance in the 2nd subplot
sns.boxplot(data=data['texture_mean'], ax=ax[1], color='grey')
ax[1].set_xlabel('texture_mean')

# draw boxplot for stores_count in the 3rd subplot
sns.boxplot(data=data['perimeter_mean'], ax=ax[2], color='yellow')
ax[2].set_xlabel('perimeter_mean')

sns.boxplot(data=data['area_mean'], ax=ax[3], color='pink')
ax[3].set_xlabel('area_mean')

sns.boxplot(data=data['smoothness_mean'], ax=ax[4], color='skyblue')
ax[4].set_xlabel('smoothness_mean')

sns.boxplot(data=data['compactness_mean'], ax=ax[5], color='purple')
ax[5].set_xlabel('compactness_mean')

sns.boxplot(data=data['concavity_mean'], ax=ax[6], color='red')
ax[6].set_xlabel('concavity_mean')

sns.boxplot(data=data['concave points_mean'], ax=ax[7], color='orange')
ax[7].set_xlabel('concave points_mean')

sns.boxplot(data=data['symmetry_mean'], ax=ax[8], color='green')
ax[8].set_xlabel('symmetry_mean')

sns.boxplot(data=data['fractal_dimension_mean'], ax=ax[9], color='blue')
ax[9].set_xlabel('fractal_dimension_mean')

# by default, you'll see x-tick label set to 0 in each subplot
# remove it by setting it to empty list
for subplot in ax:
    subplot.set_xticklabels([])


plt.show()

# Memilih kolom-kolom yang ingin dilihat hubungannya
selected_columns = ['radius_mean','texture_mean','perimeter_mean','area_mean',
                    'smoothness_mean','compactness_mean','concavity_mean','concave points_mean',
                    'symmetry_mean','fractal_dimension_mean']

selected_data = data[selected_columns]

# Membuat pair plot untuk melihat hubungan antar variabel
sns.pairplot(selected_data)
plt.suptitle('Pair Plot: Hubungan Antara Variabel-variabel Numerik')
plt.show()

"""**Cek Missing Value**"""

#Cek missing value
data.isnull().sum()

"""**Transpose Data**"""

identify = pd.DataFrame({
    'Data Kosong': data.isnull().sum(),
    'Data Duplikat': data.duplicated().sum(),
    'Data NaNN': data.isna().sum(),
    'Type Data': data.dtypes})
identify

data.transpose()

"""## **One-Hot Encoding**"""

#Melabeli data yang ada di variabel kategorik menjadi numerik
data['diagnosis']= data[['diagnosis']].apply(LabelEncoder().fit_transform)

#view udpated DataFr
data

"""## **Feature Scalling**"""

# Import Library
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Inisialisasi MinMaxScaler
scaler = MinMaxScaler()

# Memilih kolom yang ingin di scalling
kolom_scaling = ['radius_mean','texture_mean','perimeter_mean','area_mean','smoothness_mean','compactness_mean','concavity_mean','concave points_mean','symmetry_mean','fractal_dimension_mean',
                 'radius_se','texture_se','perimeter_se','area_se','smoothness_se','compactness_se','concavity_se','concave points_se','symmetry_se','fractal_dimension_se',
                 'radius_worst','texture_worst','perimeter_worst','area_worst','smoothness_worst','compactness_worst','concavity_worst','concave points_worst','symmetry_worst','fractal_dimension_worst']

# Melakukan scalling pada kolom yang telah ditentukan
data[kolom_scaling] = scaler.fit_transform(data[kolom_scaling])
data_scal = data
data_scal

import pickle

fillname14="scaler_model.sav"
pickle.dump(scaler, open(fillname14,"wb"))

"""## **Data Cleansing**"""

#Menghilangkan variabel yang tidak dibutuhkan
data1=data.drop(columns=['id'], inplace=False,axis=1)
data1=data1.drop(columns=['Unnamed: 32'], inplace=False,axis=1)
data1=data1.drop(columns=['radius_se'], inplace=False,axis=1)
data1=data1.drop(columns=['texture_se'], inplace=False,axis=1)
data1=data1.drop(columns=['perimeter_se'], inplace=False,axis=1)
data1=data1.drop(columns=['area_se'], inplace=False,axis=1)
data1=data1.drop(columns=['smoothness_se'], inplace=False,axis=1)
data1=data1.drop(columns=['compactness_se'], inplace=False,axis=1)
data1=data1.drop(columns=['concavity_se'], inplace=False,axis=1)
data1=data1.drop(columns=['concave points_se'], inplace=False,axis=1)
data1=data1.drop(columns=['symmetry_se'], inplace=False,axis=1)
data1=data1.drop(columns=['fractal_dimension_se'], inplace=False,axis=1)
data1=data1.drop(columns=['radius_worst'], inplace=False,axis=1)
data1=data1.drop(columns=['texture_worst'], inplace=False,axis=1)
data1=data1.drop(columns=['perimeter_worst'], inplace=False,axis=1)
data1=data1.drop(columns=['area_worst'], inplace=False,axis=1)
data1=data1.drop(columns=['smoothness_worst'], inplace=False,axis=1)
data1=data1.drop(columns=['compactness_worst'], inplace=False,axis=1)
data1=data1.drop(columns=['concavity_worst'], inplace=False,axis=1)
data1=data1.drop(columns=['concave points_worst'], inplace=False,axis=1)
data1=data1.drop(columns=['symmetry_worst'], inplace=False,axis=1)
data1=data1.drop(columns=['fractal_dimension_worst'], inplace=False,axis=1)
data1

data1.describe()

"""## **Telaah Data**"""

import matplotlib.pyplot as plt

plt.figure(figsize=(22,10))
data1.boxplot()
plt.show()

# Membuat pair plot untuk melihat hubungan antar variabel
sns.scatterplot(data1)
plt.suptitle('Scatter Plot : Apakah ada outlier ?')
plt.show()

identify = pd.DataFrame({
    'Data Kosong': data1.isnull().sum(),
    'Data Duplikat': data1.duplicated().sum(),
    'Data NaNN': data1.isna().sum(),
    'Type Data': data1.dtypes})
identify

X = data1.drop(columns='diagnosis')
y = data1['diagnosis']

columns = ['radius_mean','texture_mean','perimeter_mean','area_mean',
            'smoothness_mean','compactness_mean','concavity_mean','concave points_mean',
            'symmetry_mean','fractal_dimension_mean']

enc = LabelEncoder()
X['radius_mean'] = enc.fit_transform(X['radius_mean'])
X['texture_mean'] = enc.fit_transform(X['texture_mean'])
X['perimeter_mean'] = enc.fit_transform(X['perimeter_mean'])
X['area_mean'] = enc.fit_transform(X['area_mean'])
X['smoothness_mean'] = enc.fit_transform(X['smoothness_mean'])
X['compactness_mean'] = enc.fit_transform(X['compactness_mean'])
X['concavity_mean'] = enc.fit_transform(X['concavity_mean'])
X['concave points_mean'] = enc.fit_transform(X['concave points_mean'])
X['symmetry_mean'] = enc.fit_transform(X['symmetry_mean'])
X['fractal_dimension_mean'] = enc.fit_transform(X['fractal_dimension_mean'])

X

"""## **Splitting Data**"""

X = data1.drop(columns='diagnosis')
y = data1.diagnosis

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

X_train

y_test

"""## **Scalling Data**"""

columns = ['radius_mean','texture_mean','perimeter_mean','area_mean',
            'smoothness_mean','compactness_mean','concavity_mean','concave points_mean',
            'symmetry_mean','fractal_dimension_mean']

scaler = StandardScaler()
X_train[columns] = scaler.fit_transform(X_train[columns])
X_test[columns] = scaler.transform(X_test[columns])

import pickle
pickle.dump(scaler, open('scaler_model2.sav','wb'))

training = pd.DataFrame(X_train)
training

training.to_excel("Data Training 70% Data1.xlsx")

testing = pd.DataFrame(X_test)
testing

testing.to_excel("Data Testing 30% Data1.xlsx")

"""## **Build Model Clustering K-Means**"""

from sklearn.cluster import KMeans
from scipy.spatial.distance import cdist
from scipy.cluster.hierarchy import dendrogram, linkage

import numpy as np
klas = ['diagnosis']
features = np.asarray(data_scal[klas])
print(features)

from sklearn.preprocessing import normalize

data1 = pd.DataFrame(normalize(data1), columns = data1.columns)
data1

x = data1.iloc[:, 0:7].values
x

diagnosis = []
for i in range(1,11):
    kmeans = KMeans(n_clusters = i, init = "k-means++", random_state = 0)
    kmeans.fit(x)
    diagnosis.append(kmeans.inertia_)
plt.plot(range(1,11),diagnosis)
plt.title("Metode Elbow")
plt.xlabel("Jumlah Cluster")
plt.ylabel("diagnosis")
plt.show()

kmeans = KMeans(n_clusters=2,init='k-means++',
               max_iter=300,n_init=10, random_state=0)
pred_y = kmeans.fit_predict(features)
data['Cluster'] = pd.DataFrame(pred_y)
data

#Menghilangkan variabel yang tidak dibutuhkan
data2=data.drop(columns=['id'], inplace=False,axis=1)
data2=data2.drop(columns=['diagnosis'], inplace=False,axis=1)
data2=data2.drop(columns=['Unnamed: 32'], inplace=False,axis=1)
data2=data2.drop(columns=['radius_se'], inplace=False,axis=1)
data2=data2.drop(columns=['texture_se'], inplace=False,axis=1)
data2=data2.drop(columns=['perimeter_se'], inplace=False,axis=1)
data2=data2.drop(columns=['area_se'], inplace=False,axis=1)
data2=data2.drop(columns=['smoothness_se'], inplace=False,axis=1)
data2=data2.drop(columns=['compactness_se'], inplace=False,axis=1)
data2=data2.drop(columns=['concavity_se'], inplace=False,axis=1)
data2=data2.drop(columns=['concave points_se'], inplace=False,axis=1)
data2=data2.drop(columns=['symmetry_se'], inplace=False,axis=1)
data2=data2.drop(columns=['fractal_dimension_se'], inplace=False,axis=1)
data2=data2.drop(columns=['radius_worst'], inplace=False,axis=1)
data2=data2.drop(columns=['texture_worst'], inplace=False,axis=1)
data2=data2.drop(columns=['perimeter_worst'], inplace=False,axis=1)
data2=data2.drop(columns=['area_worst'], inplace=False,axis=1)
data2=data2.drop(columns=['smoothness_worst'], inplace=False,axis=1)
data2=data2.drop(columns=['compactness_worst'], inplace=False,axis=1)
data2=data2.drop(columns=['concavity_worst'], inplace=False,axis=1)
data2=data2.drop(columns=['concave points_worst'], inplace=False,axis=1)
data2=data2.drop(columns=['symmetry_worst'], inplace=False,axis=1)
data2=data2.drop(columns=['fractal_dimension_worst'], inplace=False,axis=1)
data2=data2.drop(columns=['Cluster'], inplace=False,axis=1)
data2

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, calinski_harabasz_score

# Standarisasi data untuk memastikan semua fitur memiliki skala yang serupa
MinMax = MinMaxScaler()
data2_scaled1 = MinMax.fit_transform(data2)
data2_scaled1

# Melakukan PCA dengan komponen utama yang diinginkan (misalnya, 2 komponen)
n_components1 = 2
pca1 = PCA(n_components=n_components1)
principal_components1 = pca1.fit_transform(data2_scaled1)
principal_components1

# Membuat dataframe baru yang berisi hasil PCA
pca_df1 = pd.DataFrame(data=principal_components1, columns=['PC1', 'PC2'])
pca_df1

# Menggabungkan hasil PCA dengan data asli
result_df1 = pd.concat([data[['diagnosis']], pca_df1], axis=1)

# Menampilkan hasil
print(result_df1)

# Using the elbow method to find the optimal number of clusters
wcss =[]
for i in range (1,11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter =300, n_init = 10, random_state = 0)
    kmeans.fit(principal_components1)
    wcss.append(kmeans.inertia_)

# Applying KMeans to the dataset with the optimal number of cluster
kmeans = KMeans(n_clusters= 2, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)
y_kmeans = kmeans.fit_predict(principal_components1)

# Visualising the clusters
plt.scatter(principal_components1[y_kmeans == 0, 0], principal_components1[y_kmeans == 0,1],s = 100, c='red', label = 'Cluster 1')
plt.scatter(principal_components1[y_kmeans == 1, 0], principal_components1[y_kmeans == 1,1],s = 100, c='blue', label = 'Cluster 2')

plt.title("Cluster Breast Cancer Brooklyn")
plt.legend()
plt.show()

# Calculate Silhouette Score
silhouette_avg = silhouette_score(principal_components1, y_kmeans)

# Print Silhouette Score
print("The average silhouette_score is :", silhouette_avg)

ch = calinski_harabasz_score(principal_components1, y_kmeans)
print("CH index:", ch)

kmeans1 = KMeans(n_clusters=2,init='k-means++',
               max_iter=300,n_init=10, random_state=0)
pred_y = kmeans1.fit_predict(features)
data2['Cluster'] = pd.DataFrame(pred_y)
data2

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, calinski_harabasz_score

X = np.asarray(data2)

# Visualisasi data
plt.scatter(X[:, 0], X[:, 1], s=100)
plt.show()

# Membuat model KMeans dengan 2 kluster
kmeans1 = KMeans(n_clusters=2)
kmeans1.fit(X)

# Mendapatkan label kluster untuk setiap titik data
y_kmeans = kmeans1.predict(X)

# Visualisasi hasil klustering
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')

centers = kmeans1.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75)
plt.show()

kmeans1 = KMeans(n_clusters=2)
label = kmeans1.fit_predict(X)
centroid = kmeans1.cluster_centers_

plt.figure(figsize=(6,6))
plt.scatter(X[:, 0], X[:, 1], c=label, s=10)
plt.scatter(centroid[:, 0], centroid[:, 1], c='black', edgecolors='k', s=100, linewidths=2)
plt.axis('equal')
plt.show()

from sklearn.metrics import silhouette_score

# Menghitung silhouette score
silhouette_avg = silhouette_score(X, kmeans1.labels_)
print("Silhouette Score:", silhouette_avg)

import pickle

fillname12="kmeans_model.sav"
pickle.dump(kmeans1, open(fillname12,"wb"))

"""## **Build Model Classification SVM**"""

# Defining parameter range
param_grid = {'C': [0.1, 1, 10, 100],
              'gamma': [1, 0.1, 0.01, 0.001],
              'kernel': ['rbf']}

grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)

# Fitting the model for grid search
grid.fit(X_train, y_train)

# Print best parameter after tuning
print("Best Parameters: ", grid.best_params_)

# Print how our model looks after hyper-parameter tuning
print("Best Estimator: ", grid.best_estimator_)
grid_predictions = grid.predict(X_test)

"""## **Evaluasi Model SVM**"""

# Print classification report
print(classification_report(y_test, grid_predictions))
print('Accuracy Score: ', accuracy_score(y_test, grid_predictions))

print ('accuracy_score: ',accuracy_score(y_test, grid_predictions))

import joblib

# Save the model to disk
model_filename = 'svm_model.sav'
joblib.dump(grid.best_estimator_, model_filename)
print(f"Model saved as {model_filename}")

hasil_prediksi = pd.DataFrame(grid_predictions)
hasil_prediksi.rename(columns={0:'Prediksi Data1'},inplace=True)
hasil_prediksi.to_excel('Prediksi_Data1.xlsx', index = None)
hasil_prediksi

Actual = pd.DataFrame(y_test)
Actual.to_excel('Actual Data1.xlsx', index = None)
Actual

"""## **Build Model Classification KNN**

*Import Library*
"""

model = KNeighborsClassifier()
model.fit(X_train, y_train)

"""# Evaluate model"""

y_pred = model.predict(X_test)
y_pred

print ('accuracy_score: ',accuracy_score(y_test,y_pred))

print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
cm

fig = plt.figure(figsize = (10, 6))
sns.heatmap(cm, annot = True, fmt = "d")

# Labeling the X-axis
plt.xlabel("Predicted Class")

# Labeling the Y-axis
plt.ylabel("True Class")

# Give a title to the graph
plt.title("Confusion Matrix Model")

"""## **Build Model Classification ANN**

# Splitting
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

# Library untuk visualisasi data
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, OneHotEncoder

#for model
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

# ANN model # Artificial Neural Network
import keras
from tensorflow.keras import models
from tensorflow.keras import layers
from tensorflow.keras import optimizers
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation

#for checking testing results
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix

from keras.callbacks import EarlyStopping
from keras.models import Sequential
from keras.layers import Dense
import numpy as np

# Definisikan model Sequential
model = Sequential()
model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compile model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Definisikan early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)

# Print summary model
model.summary()

# Training model dengan early stopping
history = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_test, y_test), callbacks=[early_stopping])

def plot_graphs(history, string):
    plt.plot(history.history[string])
    plt.plot(history.history['val_'+string])
    plt.xlabel('epoch')
    plt.xlabel(string)
    plt.legend([string, "val_"+string])
    plt.show()

plot_graphs(history, 'accuracy')
plot_graphs(history, 'loss')

"""**Evaluasi Model**"""

evaluation = model.evaluate(X_test, y_test)

y_pred = np.around(model.predict(X_test)).astype(np.int32)

y_pred

y_test

print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
cm

fig = plt.figure(figsize = (10, 6))
sns.heatmap(cm, annot = True, fmt = "d", cmap= 'Greens')

# Labeling the X-axis
plt.xlabel("Predicted Class")

# Labeling the Y-axis
plt.ylabel("True Class")

# Give a title to the graph
plt.title("Confusion Matrix Model with ANN")

hasil_prediksi = pd.DataFrame(y_pred)
hasil_prediksi.rename(columns={0:'Prediksi'},inplace=True)
hasil_prediksi.to_excel('Prediksi.xlsx', index = None)
hasil_prediksi

Actual = pd.DataFrame(y_test)
Actual.to_excel('Actual.xlsx', index = None)
Actual

import pickle
filename = 'KlasifikasiBreastCancerBrookly_model.sav'
pickle.dump(model, open(filename,'wb'))

